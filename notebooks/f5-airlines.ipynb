{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from  pandasgui import show\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import cross_val_predict\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, average_precision_score, precision_recall_curve\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "train_data = pd.read_csv('../airline_passenger_satisfaction.csv')\n",
    "print(train_data.info())\n",
    "# primeros_100_registros = train_data.iloc[:100]\n",
    "# test_data = pd.DataFrame(primeros_100_registros)\n",
    "primer_registro = train_data.iloc[0]\n",
    "test_data = pd.DataFrame([primer_registro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtenemos un arreglo con los nombres de las variables segun su tipo\n",
    "imputer_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]\n",
    "categorical_cols = [cname for cname in train_data.columns if train_data[cname].dtype == \"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean' )\n",
    "imputer.fit(train_data[imputer_cols])\n",
    "train_data[imputer_cols] = imputer.transform(train_data[imputer_cols])\n",
    "test_data[imputer_cols] = imputer.transform(test_data[imputer_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completamos valores nulos  en las columnas categoricas con la moda\n",
    "def fill_null_with_mode(column, train_df, test_df):\n",
    "    moda = train_df[column].mode().iloc[0]\n",
    "    train_df[column] = train_df[column].fillna(moda)\n",
    "    test_df[column] = test_df[column].fillna(moda)\n",
    "\n",
    "# Aplicar la función de llenado de valores nulos\n",
    "fill_null_with_mode(categorical_cols, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparamos los datos para dividirlos\n",
    "train_data.drop([\"Unnamed: 0\" ,\"id\"] , axis = 1 ,inplace = True)\n",
    "test_data.drop([\"Unnamed: 0\" ,\"id\",\"satisfaction\"] , axis = 1 ,inplace = True)\n",
    "# TODO no estoy seguro si debo elimiar satisfaction de test\n",
    "X = train_data.drop(\"satisfaction\" , axis =1 )\n",
    "y = train_data[\"satisfaction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding y escaling\n",
    "\n",
    "numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n",
    "print(numerical_cols)\n",
    "categorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]\n",
    "print(categorical_cols)\n",
    "boolean_cols = [cname for cname in X.columns if X[cname].dtype == 'bool']\n",
    "print(boolean_cols)\n",
    "\n",
    "# Scale numerical data to have mean=0 and variance=1\n",
    "numerical_transformer = Pipeline(steps=[('scaler', MinMaxScaler())])\n",
    "\n",
    "# One-hot encode categorical data\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(drop='if_binary', handle_unknown='ignore',sparse=False))])\n",
    "\n",
    "# Combine preprocessing\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)],\n",
    "        remainder='passthrough')\n",
    "\n",
    "# Apply preprocessing\n",
    "X = ct.fit_transform(X)\n",
    "test_data = ct.transform(test_data)\n",
    "\n",
    "# Print new shape\n",
    "print('Training set shape:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validacion cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el pipeline con los parametros del grid research\n",
    "# my_pipeline = Pipeline(steps=[\n",
    "#     ('model', XGBClassifier(**clf_best_params[\"LGBM\"], random_state=0))\n",
    "# ])\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    ('model', RandomForestClassifier(n_estimators=50))\n",
    "])\n",
    "\n",
    "# Realizar la validación cruzada y obtener las probabilidades y los scores\n",
    "proba_predictions = cross_val_predict(my_pipeline, X, y, cv=10, method='predict_proba')\n",
    "accuracy_scores = cross_val_predict(my_pipeline, X, y, cv=10, method='predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular promedio de las probabilidades de la clase positiva\n",
    "preds = proba_predictions[:, 1].mean()\n",
    "\n",
    "# Codificar las etiquetas de clase en un formato binario\n",
    "lb = LabelBinarizer()\n",
    "y_bin = lb.fit_transform(y)\n",
    "\n",
    "# Calcular matriz de confusión\n",
    "confusion = confusion_matrix(y, accuracy_scores)\n",
    "\n",
    "# Calcular precisión\n",
    "precision = precision_score(y, accuracy_scores, pos_label='satisfied')\n",
    "\n",
    "# Calcular recuperación\n",
    "recall = recall_score(y, accuracy_scores, pos_label='satisfied')\n",
    "\n",
    "# Calcular puntaje F1\n",
    "f1 = f1_score(y, accuracy_scores, pos_label='satisfied')\n",
    "\n",
    "# Calcular AUC-ROC\n",
    "roc_auc = roc_auc_score(y, proba_predictions[:, 1])\n",
    "\n",
    "# Calcular AUC-PR\n",
    "pr_auc = average_precision_score(y_bin, proba_predictions[:, 1])  # Usar y_bin\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Average probability:\", preds)\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(confusion)\n",
    "print(\"Precisión:\", precision)\n",
    "print(\"Recuperación:\", recall)\n",
    "print(\"Puntaje F1:\", f1)\n",
    "print(\"AUC-ROC:\", roc_auc)\n",
    "print(\"AUC-PR:\", pr_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y,train_size=0.8,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=50)\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular promedio de las probabilidades de la clase positiva\n",
    "preds = proba_predictions[:, 1].mean()\n",
    "\n",
    "# Calcular promedio del score de precisión\n",
    "average_accuracy = accuracy_score(y, accuracy_scores)\n",
    "\n",
    "#Matriz\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcula la precisión\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Calcula el recall\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Calcula el F1-Score\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Average probability:\", preds)\n",
    "print(\"Average accuracy:\", average_accuracy)\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix)\n",
    "print(f\"Precisión: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average probability**  \n",
    "Esto nos indica que de la cantidad de clientes satisfechos (satisfied) hay un 43.35% del total general , por lo tanto lo restante serian los que estan como \"neutral or dissatisfied\".\n",
    "\n",
    "**Average accuracy**  \n",
    "Aqui podemos observar que en cuanto a predicciones correctas, es decir que nuestro modelo cuenta con un 96.15% de acertividad en cuanto a las pruebas realizadas.\n",
    "\n",
    "**Matriz de confusión**  \n",
    "La matriz de confusión es la variable que nos permite determinar la capacidad del modelo para evitar clasificar incorrectamente las instancias negativas como positivas. A diferencia de la precisión uqe nos da una vision general de la acertividad , la matriz de confusión se utiliza para saber la categorizacion de las predicciones.\n",
    "\n",
    "En esta métrica nos indica cuantos verdaderos positivos hay (11510) es decir que la mayoria de resultados aqui estan estado \"satisfied\"; Falsos positivos aqui se puede ver que el modelo interpretó 266 clientes que en realidad estan en estado  \"neutral or dissatisfied\" pero que el modelo tomó como \"satisfied\"; En cuanto a 579 , nos indica que el modelo interpreta estos datos como falsos negativos , que quiere decir que en realidad estos clientes estan como \"satisfied\" pero el modelo los tomó como \"neutral or dissatisfied\"; por último 8426 que significa que aqui estan los verdaderos negativos , que aqui todos los clientes estan como \"neutral or dissatisfied\".\n",
    "\n",
    "**Precisión**  \n",
    "Esto nos indica que de toda la base de datos el 96% es acertado con respecto a predicciones positivas, a diferencia de ka atriz de confusión , la precisión no tiene en cuenta estos falsos positivos y se centra en la proporción de predicciones positivas correctas en relación con todas las predicciones positivas. \n",
    "\n",
    "**Recall**  \n",
    "El recall responde a la pregunta: \"De todos los casos positivos reales, ¿cuántos de ellos el modelo fue capaz de identificar correctamente?\". Es una métrica importante en problemas donde la detección de todos los casos positivos es crítica, como en la detección temprana de enfermedades o la identificación de fraudes. En este caso se puede determinar que el 96% de de los casos verdaderos positivos es correctamente identificado por el modelo.\n",
    "\n",
    "**F1-Score**  \n",
    "El F1-Score es útil cuando deseamos tener un equilibrio entre la precisión y el recall. Proporciona una puntuación que combina ambas métricas y es útil cuando ninguna de las dos métricas por sí sola es suficiente. En nuestro caso indica que está cerca al 1.0 entonces indica un buen equilibrio entre la precisión y el recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene la importancia de las características\n",
    "importancia_caracteristicas = classifier.feature_importances_\n",
    "\n",
    "# Puedes imprimir la importancia de cada característica\n",
    "for i, importancia in enumerate(importancia_caracteristicas):\n",
    "    print(f'Característica {i}: {importancia}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtén las probabilidades de predicción del modelo (ya calculadas durante la validación cruzada)\n",
    "probs = proba_predictions  # No necesitas llamar a predict_proba\n",
    "\n",
    "# Crea una lista binaria de etiquetas, 1 para \"satisfied\" y 0 para \"neutral or dissatisfied\"\n",
    "y_true_binary = [1 if label == \"satisfied\" else 0 for label in y]\n",
    "\n",
    "# Calcula la curva ROC especificando pos_label=1\n",
    "fpr, tpr, umbrales_roc = roc_curve(y_true_binary, probs[:, 1], pos_label=1)\n",
    "\n",
    "# Calcula el área bajo la curva ROC (AUC-ROC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Calcula la curva PR\n",
    "precision, recall, umbrales_pr = precision_recall_curve(y_true_binary, probs[:, 1])\n",
    "\n",
    "# Calcula el área bajo la curva PR (AUC-PR)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Graficar la curva ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='Curva ROC (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Graficar la curva PR\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='Curva PR (AUC = %0.2f)' % pr_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recuperación')\n",
    "plt.ylabel('Precisión')\n",
    "plt.title('Curva de Precisión-Recuperación')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisis de curva ROC \n",
    "\n",
    "En la curva de ROC podremos notar como el modelo tiene un 0.99 de acertividad a la hora de clasificar, sabiendo distinguir correctamente entre valores correctos e incorrectos con respecto al humbral. La curva de satisfechos y la curva de no satisfechos no se superponen casi en lo absoluto, permitiendo al modelo una facil identificacion y teniendo un minimo error al distinguir entre uno o el otro. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F5-Airlines-CuSJDrj7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

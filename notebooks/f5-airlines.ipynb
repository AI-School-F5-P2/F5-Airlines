{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from  pandasgui import show\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import cross_val_predict\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 103904 entries, 0 to 103903\n",
      "Data columns (total 25 columns):\n",
      " #   Column                             Non-Null Count   Dtype  \n",
      "---  ------                             --------------   -----  \n",
      " 0   Unnamed: 0                         103904 non-null  int64  \n",
      " 1   id                                 103904 non-null  int64  \n",
      " 2   Gender                             103904 non-null  object \n",
      " 3   Customer Type                      103904 non-null  object \n",
      " 4   Age                                103904 non-null  int64  \n",
      " 5   Type of Travel                     103904 non-null  object \n",
      " 6   Class                              103904 non-null  object \n",
      " 7   Flight Distance                    103904 non-null  int64  \n",
      " 8   Inflight wifi service              103904 non-null  int64  \n",
      " 9   Departure/Arrival time convenient  103904 non-null  int64  \n",
      " 10  Ease of Online booking             103904 non-null  int64  \n",
      " 11  Gate location                      103904 non-null  int64  \n",
      " 12  Food and drink                     103904 non-null  int64  \n",
      " 13  Online boarding                    103904 non-null  int64  \n",
      " 14  Seat comfort                       103904 non-null  int64  \n",
      " 15  Inflight entertainment             103904 non-null  int64  \n",
      " 16  On-board service                   103904 non-null  int64  \n",
      " 17  Leg room service                   103904 non-null  int64  \n",
      " 18  Baggage handling                   103904 non-null  int64  \n",
      " 19  Checkin service                    103904 non-null  int64  \n",
      " 20  Inflight service                   103904 non-null  int64  \n",
      " 21  Cleanliness                        103904 non-null  int64  \n",
      " 22  Departure Delay in Minutes         103904 non-null  int64  \n",
      " 23  Arrival Delay in Minutes           103594 non-null  float64\n",
      " 24  satisfaction                       103904 non-null  object \n",
      "dtypes: float64(1), int64(19), object(5)\n",
      "memory usage: 19.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "train_data = pd.read_csv('../data/airline_passenger_satisfaction.csv')\n",
    "print(train_data.info())\n",
    "# primeros_100_registros = train_data.iloc[:100]\n",
    "# test_data = pd.DataFrame(primeros_100_registros)\n",
    "primer_registro = train_data.iloc[0]\n",
    "test_data = pd.DataFrame([primer_registro])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtenemos un arreglo con los nombres de las variables segun su tipo\n",
    "imputer_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]\n",
    "categorical_cols = [cname for cname in train_data.columns if train_data[cname].dtype == \"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean' )\n",
    "imputer.fit(train_data[imputer_cols])\n",
    "train_data[imputer_cols] = imputer.transform(train_data[imputer_cols])\n",
    "test_data[imputer_cols] = imputer.transform(test_data[imputer_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completamos valores nulos  en las columnas categoricas con la moda\n",
    "def fill_null_with_mode(column, train_df, test_df):\n",
    "    moda = train_df[column].mode().iloc[0]\n",
    "    train_df[column] = train_df[column].fillna(moda)\n",
    "    test_df[column] = test_df[column].fillna(moda)\n",
    "\n",
    "# Aplicar la funci√≥n de llenado de valores nulos\n",
    "fill_null_with_mode(categorical_cols, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparamos los datos para dividirlos\n",
    "train_data.drop([\"Unnamed: 0\" ,\"id\"] , axis = 1 ,inplace = True)\n",
    "test_data.drop([\"Unnamed: 0\" ,\"id\",\"satisfaction\"] , axis = 1 ,inplace = True)\n",
    "# TODO no estoy seguro si debo elimiar satisfaction de test\n",
    "X = train_data.drop(\"satisfaction\" , axis =1 )\n",
    "y = train_data[\"satisfaction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Flight Distance', 'Inflight wifi service', 'Departure/Arrival time convenient', 'Ease of Online booking', 'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort', 'Inflight entertainment', 'On-board service', 'Leg room service', 'Baggage handling', 'Checkin service', 'Inflight service', 'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\n",
      "['Gender', 'Customer Type', 'Type of Travel', 'Class']\n",
      "[]\n",
      "Training set shape: (103904, 24)\n"
     ]
    }
   ],
   "source": [
    "# encoding y escaling\n",
    "\n",
    "numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n",
    "print(numerical_cols)\n",
    "categorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]\n",
    "print(categorical_cols)\n",
    "boolean_cols = [cname for cname in X.columns if X[cname].dtype == 'bool']\n",
    "print(boolean_cols)\n",
    "\n",
    "# Scale numerical data to have mean=0 and variance=1\n",
    "numerical_transformer = Pipeline(steps=[('scaler', MinMaxScaler())])\n",
    "\n",
    "# One-hot encode categorical data\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(drop='if_binary', handle_unknown='ignore',sparse=False))])\n",
    "\n",
    "# Combine preprocessing\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)],\n",
    "        remainder='passthrough')\n",
    "\n",
    "# Apply preprocessing\n",
    "X = ct.fit_transform(X)\n",
    "test_data = ct.transform(test_data)\n",
    "\n",
    "# Print new shape\n",
    "print('Training set shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y,train_size=0.8,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': range(20, 101, 20),\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion' : ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "list(param_grid['n_estimators'])\n",
    "\n",
    "scoring = 'accuracy'\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=scoring, cv=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 90 candidates, totalling 180 fits\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=20; total time=   1.2s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=20; total time=   1.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=40; total time=   2.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=40; total time=   2.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=60; total time=   3.8s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=60; total time=   3.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=80; total time=   4.7s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=80; total time=   4.8s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=100; total time=   6.7s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=2, n_estimators=100; total time=   6.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=20; total time=   2.2s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=20; total time=   1.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=40; total time=   3.2s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=40; total time=   3.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=60; total time=   4.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=60; total time=   4.7s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=80; total time=   6.5s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=80; total time=   6.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=100; total time=   7.7s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=5, n_estimators=100; total time=   8.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=20; total time=   1.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=20; total time=   2.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=40; total time=   5.1s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=40; total time=   5.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=60; total time=   5.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=60; total time=   4.7s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=80; total time=   6.3s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=80; total time=   6.4s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=100; total time=   7.9s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_split=10, n_estimators=100; total time=   8.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=20; total time=   1.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=40; total time=   1.7s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=40; total time=   1.7s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=60; total time=   2.4s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=60; total time=   2.5s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=80; total time=   3.4s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=80; total time=   3.3s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=2, n_estimators=100; total time=   3.7s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=20; total time=   0.8s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=20; total time=   0.7s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=40; total time=   1.7s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=60; total time=   2.4s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=60; total time=   2.4s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=60; total time=   2.5s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=60; total time=   2.7s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=80; total time=   3.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=80; total time=   3.1s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_split=10, n_estimators=100; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=20; total time=   1.2s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=20; total time=   1.2s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=40; total time=   2.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=40; total time=   2.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=60; total time=   3.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=60; total time=   3.6s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=80; total time=   4.7s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=80; total time=   4.6s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=100; total time=   5.9s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=2, n_estimators=100; total time=   6.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=20; total time=   1.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=20; total time=   1.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=40; total time=   2.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=40; total time=   2.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=60; total time=   3.8s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=60; total time=   3.7s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=80; total time=   5.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=80; total time=   5.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=100; total time=   6.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=5, n_estimators=100; total time=   5.8s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=20; total time=   1.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=20; total time=   1.3s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=40; total time=   2.4s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=40; total time=   2.5s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=60; total time=   3.9s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=60; total time=   3.7s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=80; total time=   4.8s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=80; total time=   4.6s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=100; total time=   5.9s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_split=10, n_estimators=100; total time=   6.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=20; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=20; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=40; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=40; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=60; total time=   5.2s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=60; total time=   5.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=80; total time=   6.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=80; total time=   6.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=100; total time=   8.5s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=2, n_estimators=100; total time=   8.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=20; total time=   1.8s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=20; total time=   1.8s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=40; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=40; total time=   3.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=60; total time=   4.9s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=60; total time=   4.8s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=80; total time=   8.4s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=80; total time=   6.3s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=100; total time=   8.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=5, n_estimators=100; total time=   8.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=20; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=20; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=40; total time=   3.1s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=40; total time=   3.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=60; total time=   4.9s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=60; total time=   4.8s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=80; total time=   6.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=80; total time=   6.4s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=100; total time=   7.8s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_split=10, n_estimators=100; total time=   7.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=60; total time=   2.4s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=60; total time=   2.5s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=60; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=60; total time=   2.4s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=100; total time=   3.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=5, n_estimators=100; total time=   4.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=20; total time=   1.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=40; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=40; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=60; total time=   2.4s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=60; total time=   2.5s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=80; total time=   2.9s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=100; total time=   3.8s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_split=10, n_estimators=100; total time=   4.2s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=20; total time=   1.4s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=20; total time=   1.4s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=40; total time=   2.6s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=40; total time=   2.5s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=60; total time=   2.7s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=60; total time=   3.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=80; total time=   3.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=80; total time=   3.1s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=20; total time=   0.8s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=20; total time=   0.8s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=40; total time=   1.6s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=40; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=60; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=60; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=80; total time=   3.1s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=80; total time=   3.2s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=100; total time=   4.3s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=5, n_estimators=100; total time=   4.4s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=20; total time=   0.9s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=40; total time=   1.7s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=40; total time=   1.5s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=60; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=60; total time=   2.3s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=80; total time=   3.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=80; total time=   3.4s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=100; total time=   4.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_split=10, n_estimators=100; total time=   3.8s\n",
      "Best hyperparameters: {'criterion': 'gini', 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best score: 0.9578095287124689\n",
      "Testing accuracy: 0.9617920215581541\n"
     ]
    }
   ],
   "source": [
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    " \n",
    "# Print the best hyperparameters and the best score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    " \n",
    "# Re-train the model with the best hyperparameters\n",
    "best_clf = grid_search.best_estimator_\n",
    "best_clf.fit(X_train, y_train)\n",
    " \n",
    "# Test the model with the best hyperparameters on the testing data\n",
    "accuracy = best_clf.score(X_test, y_test)\n",
    "print(\"Testing accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average probability: 0.4336235371111795\n"
     ]
    }
   ],
   "source": [
    "# Crear el pipeline con los parametros del grid search\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    ('model', RandomForestClassifier(criterion = 'gini' ,max_depth= None, min_samples_split=2, n_estimators=100))\n",
    "])\n",
    "\n",
    "# Realizar la validaci√≥n cruzada y obtener las probabilidades y los scores\n",
    "proba_predictions = cross_val_predict(my_pipeline, X_train, y_train, cv=5, method='predict_proba')\n",
    "accuracy_scores = cross_val_predict(my_pipeline, X_train, y_train, cv=5, method='predict')\n",
    "\n",
    "# Calcular promedio de las probabilidades de la clase positiva\n",
    "preds = proba_predictions[:, 1].mean()\n",
    "\n",
    "# Calcular promedio del score de precisi√≥n\n",
    "average_accuracy = accuracy_score(y, accuracy_scores)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Average probability:\", preds)\n",
    "# print(\"Average accuracy:\", average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(criterion = 'gini', max_depth= None, min_samples_split=2, n_estimators=100)\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral or dissatisfied'], dtype=object)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average probability: 0.4335077571604558\n",
      "Average accuracy: 0.9623017400677548\n",
      "Matriz de Confusi√≥n:\n",
      " [[11505   271]\n",
      " [  550  8455]]\n",
      "Precisi√≥n: 0.96\n",
      "Recall: 0.96\n",
      "F1-Score: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Calcular promedio de las probabilidades de la clase positiva\n",
    "preds = proba_predictions[:, 1].mean()\n",
    "\n",
    "# Calcular promedio del score de precisi√≥n\n",
    "average_accuracy = accuracy_score(y, accuracy_scores)\n",
    "\n",
    "#Matriz\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcula la precisi√≥n\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Calcula el recall\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Calcula el F1-Score\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Average probability:\", preds)\n",
    "print(\"Average accuracy:\", average_accuracy)\n",
    "print(\"Matriz de Confusi√≥n:\\n\", conf_matrix)\n",
    "print(f\"Precisi√≥n: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average probability**  \n",
    "Esto nos indica que de la cantidad de clientes satisfechos (satisfied) hay un 43.35% del total general , por lo tanto lo restante serian los que estan como \"neutral or dissatisfied\".\n",
    "\n",
    "**Average accuracy**  \n",
    "Aqui podemos observar que en cuanto a predicciones correctas, es decir que nuestro modelo cuenta con un 96.15% de acertividad en cuanto a las pruebas realizadas.\n",
    "\n",
    "**Matriz de confusi√≥n**  \n",
    "La matriz de confusi√≥n es la variable que nos permite determinar la capacidad del modelo para evitar clasificar incorrectamente las instancias negativas como positivas. A diferencia de la precisi√≥n uqe nos da una vision general de la acertividad , la matriz de confusi√≥n se utiliza para saber la categorizacion de las predicciones.\n",
    "\n",
    "En esta m√©trica nos indica cuantos verdaderos positivos hay (11510) es decir que la mayoria de resultados aqui estan estado \"satisfied\"; Falsos positivos aqui se puede ver que el modelo interpret√≥ 266 clientes que en realidad estan en estado  \"neutral or dissatisfied\" pero que el modelo tom√≥ como \"satisfied\"; En cuanto a 579 , nos indica que el modelo interpreta estos datos como falsos negativos , que quiere decir que en realidad estos clientes estan como \"satisfied\" pero el modelo los tom√≥ como \"neutral or dissatisfied\"; por √∫ltimo 8426 que significa que aqui estan los verdaderos negativos , que aqui todos los clientes estan como \"neutral or dissatisfied\".\n",
    "\n",
    "**Precisi√≥n**  \n",
    "Esto nos indica que de toda la base de datos el 96% es acertado con respecto a predicciones positivas, a diferencia de ka atriz de confusi√≥n , la precisi√≥n no tiene en cuenta estos falsos positivos y se centra en la proporci√≥n de predicciones positivas correctas en relaci√≥n con todas las predicciones positivas. \n",
    "\n",
    "**Recall**  \n",
    "El recall responde a la pregunta: \"De todos los casos positivos reales, ¬øcu√°ntos de ellos el modelo fue capaz de identificar correctamente?\". Es una m√©trica importante en problemas donde la detecci√≥n de todos los casos positivos es cr√≠tica, como en la detecci√≥n temprana de enfermedades o la identificaci√≥n de fraudes. En este caso se puede determinar que el 96% de de los casos verdaderos positivos es correctamente identificado por el modelo.\n",
    "\n",
    "**F1-Score**  \n",
    "El F1-Score es √∫til cuando deseamos tener un equilibrio entre la precisi√≥n y el recall. Proporciona una puntuaci√≥n que combina ambas m√©tricas y es √∫til cuando ninguna de las dos m√©tricas por s√≠ sola es suficiente. En nuestro caso indica que est√° cerca al 1.0 entonces indica un buen equilibrio entre la precisi√≥n y el recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene la importancia de las caracter√≠sticas\n",
    "importancia_caracteristicas = classifier.feature_importances_\n",
    "\n",
    "# Puedes imprimir la importancia de cada caracter√≠stica\n",
    "for i, importancia in enumerate(importancia_caracteristicas):\n",
    "    print(f'Caracter√≠stica {i}: {importancia}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapea las etiquetas a valores binarios\n",
    "y_true_binary = [1 if label == \"satisfied\" else 0 for label in y_test]\n",
    "\n",
    "# Obt√©n las probabilidades de predicci√≥n del modelo (por ejemplo, un clasificador de bosque aleatorio)\n",
    "probs = classifier.predict_proba(X_test)  # X_test son las caracter√≠sticas de prueba\n",
    "\n",
    "# Calcula la curva ROC especificando pos_label=1\n",
    "fpr, tpr, umbrales = roc_curve(y_true_binary, probs[:, 1], pos_label=1)\n",
    "\n",
    "# Calcula el √°rea bajo la curva ROC (AUC-ROC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Grafica la curva ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisis de curva ROC \n",
    "\n",
    "En la curva de ROC podremos notar como el modelo tiene un 0.99 de acertividad a la hora de clasificar, sabiendo distinguir correctamente entre valores correctos e incorrectos con respecto al humbral. La curva de satisfechos y la curva de no satisfechos no se superponen casi en lo absoluto, permitiendo al modelo una facil identificacion y teniendo un minimo error al distinguir entre uno o el otro. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F5-Airlines-CuSJDrj7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

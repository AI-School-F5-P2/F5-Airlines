{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from  pandasgui import show\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import cross_val_predict\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "train_data = pd.read_csv(\"../airline_passenger_satisfaction.csv\")\n",
    "# primeros_100_registros = train_data.iloc[:100]\n",
    "# test_data = pd.DataFrame(primeros_100_registros)\n",
    "#primer_registro = train_data.iloc[0]\n",
    "#test_data = pd.DataFrame([primer_registro])\n",
    "train_data.drop([\"Unnamed: 0\" ,\"id\"] , axis = 1 ,inplace = True)\n",
    "print(train_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtenemos un arreglo con los nombres de las variables segun su tipo\n",
    "imputer_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]\n",
    "categorical_cols = [cname for cname in train_data.columns if train_data[cname].dtype == \"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean' )\n",
    "imputer.fit(train_data[imputer_cols])\n",
    "train_data[imputer_cols] = imputer.transform(train_data[imputer_cols])\n",
    "#test_data[imputer_cols] = imputer.transform(test_data[imputer_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completamos valores nulos  en las columnas categoricas con la moda\n",
    "def fill_null_with_mode(column, train_df):#test_df\n",
    "    moda = train_df[column].mode().iloc[0]\n",
    "    train_df[column] = train_df[column].fillna(moda)\n",
    "    #test_df[column] = test_df[column].fillna(moda)\n",
    "\n",
    "# Aplicar la función de llenado de valores nulos\n",
    "fill_null_with_mode(categorical_cols, train_data)#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparamos los datos para dividirlos\n",
    "#train_data.drop([\"Unnamed: 0\" ,\"id\"] , axis = 1 ,inplace = True)\n",
    "#test_data.drop([\"Unnamed: 0\" ,\"id\",\"satisfaction\"] , axis = 1 ,inplace = True)\n",
    "# TODO no estoy seguro si debo elimiar satisfaction de test\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una copia de los datos originales para evitar modificarlos\n",
    "X = train_data.drop(\"satisfaction\", axis=1)\n",
    "y = train_data[\"satisfaction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Lista de columnas numéricas\n",
    "numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Lista de columnas categóricas en X\n",
    "categorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]\n",
    "\n",
    "# Escalar las características numéricas\n",
    "numerical_transformer = Pipeline(steps=[('scaler', MinMaxScaler())])\n",
    "\n",
    "# Codificar one-hot las características categóricas\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False))])\n",
    "\n",
    "# Combinar transformaciones para X\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "\n",
    "# Mapear 'satisfied' a 1 y 'neutral' a 0\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Aplicar las transformaciones a X\n",
    "X = preprocessor.fit_transform(X)\n",
    "\n",
    "# Imprimir nueva forma de X\n",
    "print('Training set shape (X):', X.shape)\n",
    "\n",
    "# Imprimir nueva forma de y\n",
    "print('Training set shape (y):', y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': range(20, 101, 20),\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion' : ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "list(param_grid['n_estimators'])\n",
    "\n",
    "scoring = 'accuracy'\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=scoring, cv=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    " \n",
    "# Print the best hyperparameters and the best score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    " \n",
    "# Re-train the model with the best hyperparameters\n",
    "best_clf = grid_search.best_estimator_\n",
    "best_clf.fit(X_train, y_train)\n",
    " \n",
    "# Test the model with the best hyperparameters on the testing data\n",
    "accuracy = best_clf.score(X_test, y_test)\n",
    "print(\"Testing accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el pipeline con los parametros del grid search\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    ('model', RandomForestClassifier(criterion = 'gini', max_depth = None, min_samples_split = 2, n_estimators = 80))\n",
    "])\n",
    "\n",
    "# Realizar la validación cruzada y obtener las probabilidades y los scores\n",
    "predicted = cross_val_predict(my_pipeline, X_train, y_train, cv=5)\n",
    "\n",
    "accuracy_scores = accuracy_score(y_train, predicted)\n",
    "confusion = confusion_matrix(y_train, predicted)\n",
    "recall = recall_score(y_train, predicted)\n",
    "f1 = f1_score(y_train, predicted)\n",
    "\n",
    "# Imprime las métricas para cada pliegue\n",
    "print(\"Accuracy:\", accuracy_scores)\n",
    "print(\"Confusion Matrix:\\n\", confusion)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(criterion = 'gini', max_depth= None, min_samples_split=2, n_estimators=100)\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier.predict(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular promedio de las probabilidades de la clase positiva\n",
    "#preds = proba_predictions[:, 1].mean()\n",
    "\n",
    "# Calcular promedio del score de precisión\n",
    "average_accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "#Matriz\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcula la precisión\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Calcula el recall\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Calcula el F1-Score\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Imprimir los resultados\n",
    "#print(\"Average probability:\", preds)\n",
    "print(\"Average accuracy:\", average_accuracy)\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix)\n",
    "print(f\"Precisión: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average probability**  \n",
    "Esto nos indica que de la cantidad de clientes satisfechos (satisfied) hay un 43.35% del total general , por lo tanto lo restante serian los que estan como \"neutral or dissatisfied\".\n",
    "\n",
    "**Average accuracy**  \n",
    "Aqui podemos observar que en cuanto a predicciones correctas, es decir que nuestro modelo cuenta con un 96.15% de acertividad en cuanto a las pruebas realizadas.\n",
    "\n",
    "**Matriz de confusión**  \n",
    "La matriz de confusión es la variable que nos permite determinar la capacidad del modelo para evitar clasificar incorrectamente las instancias negativas como positivas. A diferencia de la precisión uqe nos da una vision general de la acertividad , la matriz de confusión se utiliza para saber la categorizacion de las predicciones.\n",
    "\n",
    "En esta métrica nos indica cuantos verdaderos positivos hay (11510) es decir que la mayoria de resultados aqui estan estado \"satisfied\"; Falsos positivos aqui se puede ver que el modelo interpretó 266 clientes que en realidad estan en estado  \"neutral or dissatisfied\" pero que el modelo tomó como \"satisfied\"; En cuanto a 579 , nos indica que el modelo interpreta estos datos como falsos negativos , que quiere decir que en realidad estos clientes estan como \"satisfied\" pero el modelo los tomó como \"neutral or dissatisfied\"; por último 8426 que significa que aqui estan los verdaderos negativos , que aqui todos los clientes estan como \"neutral or dissatisfied\".\n",
    "\n",
    "**Precisión**  \n",
    "Esto nos indica que de toda la base de datos el 96% es acertado con respecto a predicciones positivas, a diferencia de ka atriz de confusión , la precisión no tiene en cuenta estos falsos positivos y se centra en la proporción de predicciones positivas correctas en relación con todas las predicciones positivas. \n",
    "\n",
    "**Recall**  \n",
    "El recall responde a la pregunta: \"De todos los casos positivos reales, ¿cuántos de ellos el modelo fue capaz de identificar correctamente?\". Es una métrica importante en problemas donde la detección de todos los casos positivos es crítica, como en la detección temprana de enfermedades o la identificación de fraudes. En este caso se puede determinar que el 96% de de los casos verdaderos positivos es correctamente identificado por el modelo.\n",
    "\n",
    "**F1-Score**  \n",
    "El F1-Score es útil cuando deseamos tener un equilibrio entre la precisión y el recall. Proporciona una puntuación que combina ambas métricas y es útil cuando ninguna de las dos métricas por sí sola es suficiente. En nuestro caso indica que está cerca al 1.0 entonces indica un buen equilibrio entre la precisión y el recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene la importancia de las características\n",
    "importancia_caracteristicas = classifier.feature_importances_\n",
    "\n",
    "# Puedes imprimir la importancia de cada característica\n",
    "for i, importancia in enumerate(importancia_caracteristicas):\n",
    "    print(f'Característica {i}: {importancia}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapea las etiquetas a valores binarios\n",
    "y_true_binary = [1 if label == \"satisfied\" else 0 for label in y_test]\n",
    "\n",
    "# Obtén las probabilidades de predicción del modelo (por ejemplo, un clasificador de bosque aleatorio)\n",
    "probs = classifier.predict_proba(X_test)  # X_test son las características de prueba\n",
    "\n",
    "# Calcula la curva ROC especificando pos_label=1\n",
    "fpr, tpr, umbrales = roc_curve(y_true_binary, probs[:, 1], pos_label=1)\n",
    "\n",
    "# Calcula el área bajo la curva ROC (AUC-ROC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Grafica la curva ROC\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisis de curva ROC \n",
    "\n",
    "En la curva de ROC podremos notar como el modelo tiene un 0.99 de acertividad a la hora de clasificar, sabiendo distinguir correctamente entre valores correctos e incorrectos con respecto al humbral. La curva de satisfechos y la curva de no satisfechos no se superponen casi en lo absoluto, permitiendo al modelo una facil identificacion y teniendo un minimo error al distinguir entre uno o el otro. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F5-Airlines-CuSJDrj7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#from  pandasgui import show\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import cross_val_predict\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, average_precision_score, precision_recall_curve\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "train_data = pd.read_csv('../data/airline_passenger_satisfaction.csv')\n",
    "print(train_data.info())\n",
    "primeros_100_registros = train_data.iloc[:100]\n",
    "test_data = pd.DataFrame(primeros_100_registros)\n",
    "primer_registro = train_data.iloc[0]\n",
    "test_data = pd.DataFrame([primer_registro])\n",
    "print(train_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtenemos un arreglo con los nombres de las variables segun su tipo\n",
    "imputer_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]\n",
    "categorical_cols = [cname for cname in train_data.columns if train_data[cname].dtype == \"object\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean' )\n",
    "imputer.fit(train_data[imputer_cols])\n",
    "train_data[imputer_cols] = imputer.transform(train_data[imputer_cols])\n",
    "test_data[imputer_cols] = imputer.transform(test_data[imputer_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completamos valores nulos  en las columnas categoricas con la moda\n",
    "def fill_null_with_mode(column, train_df):#test_df\n",
    "    moda = train_df[column].mode().iloc[0]\n",
    "    train_df[column] = train_df[column].fillna(moda)\n",
    "    #test_df[column] = test_df[column].fillna(moda)\n",
    "\n",
    "# Aplicar la función de llenado de valores nulos\n",
    "fill_null_with_mode(categorical_cols, train_data)#test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparamos los datos para dividirlos\n",
    "train_data.drop([\"Unnamed: 0\" ,\"id\"] , axis = 1 ,inplace = True)\n",
    "test_data.drop([\"Unnamed: 0\" ,\"id\",\"satisfaction\"] , axis = 1 ,inplace = True)\n",
    "# TODO no estoy seguro si debo elimiar satisfaction de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una copia de los datos originales para evitar modificarlos\n",
    "X = train_data.drop(\"satisfaction\", axis=1)\n",
    "y = train_data[\"satisfaction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Lista de columnas numéricas\n",
    "numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Lista de columnas categóricas en X\n",
    "categorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]\n",
    "\n",
    "# Escalar las características numéricas\n",
    "numerical_transformer = Pipeline(steps=[('scaler', MinMaxScaler())])\n",
    "\n",
    "# Codificar one-hot las características categóricas\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse=False))])\n",
    "\n",
    "# Combinar transformaciones para X\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "# Aplicar las transformaciones a X\n",
    "X = preprocessor.fit_transform(X)\n",
    "\n",
    "# Imprimir nueva forma de X\n",
    "print('Training set shape (X):', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': range(20, 101, 20),\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion' : ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "scoring = 'accuracy'\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=scoring, cv=2, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(X, y)\n",
    " \n",
    "# Print the best hyperparameters and the best score\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    " \n",
    "# Re-train the model with the best hyperparameters\n",
    "best_clf = grid_search.best_estimator_\n",
    "best_clf.fit(X, y)\n",
    " \n",
    "# Test the model with the best hyperparameters on the testing data\n",
    "accuracy = best_clf.score(X, y)\n",
    "print(\"Testing accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validacion cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el pipeline con los parametros del grid search\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    ('model', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "my_pipeline.named_steps['model'].set_params(**grid_search.best_params_)\n",
    "\n",
    "# Realizar la validación cruzada y obtener las probabilidades y los scores\n",
    "proba_predictions = cross_val_predict(my_pipeline, X, y, cv=10, method='predict_proba')\n",
    "accuracy_scores = cross_val_predict(my_pipeline, X, y, cv=10, method='predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular promedio de las probabilidades de la clase positiva\n",
    "preds = proba_predictions[:, 1].mean()\n",
    "\n",
    "# Codificar las etiquetas de clase en un formato binario\n",
    "lb = LabelBinarizer()\n",
    "y_bin = lb.fit_transform(y)\n",
    "\n",
    "# Calcular matriz de confusión\n",
    "confusion = confusion_matrix(y, accuracy_scores)\n",
    "\n",
    "# Calcular precisión\n",
    "precision = precision_score(y, accuracy_scores, pos_label='satisfied')\n",
    "\n",
    "# Calcular recuperación\n",
    "recall = recall_score(y, accuracy_scores, pos_label='satisfied')\n",
    "\n",
    "# Calcular puntaje F1\n",
    "f1 = f1_score(y, accuracy_scores, pos_label='satisfied')\n",
    "\n",
    "# Calcular AUC-ROC\n",
    "roc_auc = roc_auc_score(y, proba_predictions[:, 1])\n",
    "\n",
    "# Calcular AUC-PR\n",
    "pr_auc = average_precision_score(y_bin, proba_predictions[:, 1])  # Usar y_bin\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Average probability:\", preds)\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(confusion)\n",
    "print(\"Precisión:\", precision)\n",
    "print(\"Recuperación:\", recall)\n",
    "print(\"Puntaje F1:\", f1)\n",
    "print(\"AUC-ROC:\", roc_auc)\n",
    "print(\"AUC-PR:\", pr_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y,train_size=0.8,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(criterion = 'gini', max_depth= None, min_samples_split=2, n_estimators=100)\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular promedio de las probabilidades de la clase positiva\n",
    "#preds = proba_predictions[:, 1].mean()\n",
    "\n",
    "# Calcular promedio del score de precisión\n",
    "average_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#Matriz\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calcula la precisión\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Calcula el recall\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Calcula el F1-Score\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Imprimir los resultados\n",
    "#print(\"Average probability:\", preds)\n",
    "print(\"Average accuracy:\", average_accuracy)\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix)\n",
    "print(f\"Precisión: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average probability**  \n",
    "Esto nos indica que de la cantidad de clientes satisfechos (satisfied) hay un 43.35% del total general , por lo tanto lo restante serian los que estan como \"neutral or dissatisfied\".\n",
    "\n",
    "**Average accuracy**  \n",
    "Aqui podemos observar que en cuanto a predicciones correctas, es decir que nuestro modelo cuenta con un 96.15% de acertividad en cuanto a las pruebas realizadas.\n",
    "\n",
    "**Matriz de confusión**  \n",
    "La matriz de confusión es la variable que nos permite determinar la capacidad del modelo para evitar clasificar incorrectamente las instancias negativas como positivas. A diferencia de la precisión uqe nos da una vision general de la acertividad , la matriz de confusión se utiliza para saber la categorizacion de las predicciones.\n",
    "\n",
    "En esta métrica nos indica cuantos verdaderos positivos hay (11510) es decir que la mayoria de resultados aqui estan estado \"satisfied\"; Falsos positivos aqui se puede ver que el modelo interpretó 266 clientes que en realidad estan en estado  \"neutral or dissatisfied\" pero que el modelo tomó como \"satisfied\"; En cuanto a 579 , nos indica que el modelo interpreta estos datos como falsos negativos , que quiere decir que en realidad estos clientes estan como \"satisfied\" pero el modelo los tomó como \"neutral or dissatisfied\"; por último 8426 que significa que aqui estan los verdaderos negativos , que aqui todos los clientes estan como \"neutral or dissatisfied\".\n",
    "\n",
    "**Precisión**  \n",
    "Esto nos indica que de toda la base de datos el 96% es acertado con respecto a predicciones positivas, a diferencia de ka atriz de confusión , la precisión no tiene en cuenta estos falsos positivos y se centra en la proporción de predicciones positivas correctas en relación con todas las predicciones positivas. \n",
    "\n",
    "**Recall**  \n",
    "El recall responde a la pregunta: \"De todos los casos positivos reales, ¿cuántos de ellos el modelo fue capaz de identificar correctamente?\". Es una métrica importante en problemas donde la detección de todos los casos positivos es crítica, como en la detección temprana de enfermedades o la identificación de fraudes. En este caso se puede determinar que el 96% de de los casos verdaderos positivos es correctamente identificado por el modelo.\n",
    "\n",
    "**F1-Score**  \n",
    "El F1-Score es útil cuando deseamos tener un equilibrio entre la precisión y el recall. Proporciona una puntuación que combina ambas métricas y es útil cuando ninguna de las dos métricas por sí sola es suficiente. En nuestro caso indica que está cerca al 1.0 entonces indica un buen equilibrio entre la precisión y el recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene la importancia de las características\n",
    "importancia_caracteristicas = classifier.feature_importances_\n",
    "\n",
    "# Puedes imprimir la importancia de cada característica\n",
    "for i, importancia in enumerate(importancia_caracteristicas):\n",
    "    print(f'Característica {i}: {importancia}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtén las probabilidades de predicción del modelo (ya calculadas durante la validación cruzada)\n",
    "probs = proba_predictions  # No necesitas llamar a predict_proba\n",
    "\n",
    "# Crea una lista binaria de etiquetas, 1 para \"satisfied\" y 0 para \"neutral or dissatisfied\"\n",
    "y_true_binary = [1 if label == \"satisfied\" else 0 for label in y]\n",
    "\n",
    "# Calcula la curva ROC especificando pos_label=1\n",
    "fpr, tpr, umbrales_roc = roc_curve(y_true_binary, probs[:, 1], pos_label=1)\n",
    "\n",
    "# Calcula el área bajo la curva ROC (AUC-ROC)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Calcula la curva PR\n",
    "precision, recall, umbrales_pr = precision_recall_curve(y_true_binary, probs[:, 1])\n",
    "\n",
    "# Calcula el área bajo la curva PR (AUC-PR)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Graficar la curva ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='Curva ROC (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Tasa de Falsos Positivos')\n",
    "plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Graficar la curva PR\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='Curva PR (AUC = %0.2f)' % pr_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recuperación')\n",
    "plt.ylabel('Precisión')\n",
    "plt.title('Curva de Precisión-Recuperación')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisis de curva ROC \n",
    "\n",
    "En la curva de ROC podremos notar como el modelo tiene un 0.99 de acertividad a la hora de clasificar, sabiendo distinguir correctamente entre valores correctos e incorrectos con respecto al humbral. La curva de satisfechos y la curva de no satisfechos no se superponen casi en lo absoluto, permitiendo al modelo una facil identificacion y teniendo un minimo error al distinguir entre uno o el otro. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tecnicas ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    ('model', GradientBoostingClassifier(n_estimators=50))\n",
    "])\n",
    "\n",
    "# Validación cruzada\n",
    "proba_predictions = cross_val_predict(my_pipeline, X, y, cv=10, method='predict_proba')\n",
    "accuracy_scores = cross_val_predict(my_pipeline, X, y, cv=10, method='predict')\n",
    "\n",
    "# Calcular promedio de las probabilidades de la clase positiva\n",
    "preds = proba_predictions[:, 1].mean()\n",
    "\n",
    "# Precisión\n",
    "average_accuracy = accuracy_score(y, accuracy_scores)\n",
    "\n",
    "print(\"Average probability:\", preds)\n",
    "print(\"Average accuracy:\", average_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "my_pipeline = Pipeline(steps=[\n",
    "    ('model', AdaBoostClassifier(n_estimators=50))\n",
    "])\n",
    "\n",
    "# Validación cruzada\n",
    "proba_predictions = cross_val_predict(my_pipeline, X, y, cv=10, method='predict_proba')\n",
    "accuracy_scores = cross_val_predict(my_pipeline, X, y, cv=10, method='predict')\n",
    "\n",
    "# Promedio de las probabilidades de la clase positiva\n",
    "preds = proba_predictions[:, 1].mean()\n",
    "\n",
    "# Precisión\n",
    "average_accuracy = accuracy_score(y, accuracy_scores)\n",
    "\n",
    "print(\"Average probability:\", preds)\n",
    "print(\"Average accuracy:\", average_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo ensemble Stackin utilizando ramdom Forest, Gradient Boosting y AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "# Entrenando los tres modelos seleccionados\n",
    "random_forest = RandomForestClassifier(n_estimators=50)\n",
    "adaboost = AdaBoostClassifier(n_estimators=50)\n",
    "gradient_boosting = GradientBoostingClassifier(n_estimators=50)\n",
    "\n",
    "# Modelo de nivel superior (meta-modelo)\n",
    "meta_classifier = RandomForestClassifier(n_estimators=50)  # Puedes cambiar esto según tus necesidades\n",
    "\n",
    "models = [\n",
    "    ('random_forest', random_forest),\n",
    "    ('adaboost', adaboost),\n",
    "    ('gradient_boosting', gradient_boosting)\n",
    "]\n",
    "\n",
    "# Modelo de stacking\n",
    "stacking_model = StackingClassifier(estimators=models, final_estimator=meta_classifier)\n",
    "\n",
    "# Entreamiento del modelo stacking\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "\n",
    "# Precisión\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Matriz de Confusión:\\n\", conf_matrix)\n",
    "\n",
    "# precisión (Precision)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "print(\"Precisión (Macro):\", precision)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "print(\"Recall (Macro):\", recall)\n",
    "\n",
    "# F1-Score\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "print(\"F1-Score (Macro):\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define un rango de tamaños de conjunto de entrenamiento\n",
    "train_sizes, train_scores, test_scores = learning_curve(my_pipeline, X, y, cv=10, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "\n",
    "# Calcula las medias y desviaciones estándar de las puntuaciones para el entrenamiento y prueba\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Grafica las curvas de aprendizaje\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Curvas de Aprendizaje\")\n",
    "plt.xlabel(\"Tamaño del Conjunto de Entrenamiento\")\n",
    "plt.ylabel(\"Puntuación (Accuracy)\")\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Entrenamiento\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Prueba\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula la diferencia entre las puntuaciones de entrenamiento y prueba para cada tamaño de conjunto de entrenamiento\n",
    "overfitting_measure = train_scores_mean - test_scores_mean\n",
    "\n",
    "# Grafica la medida de overfitting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Medida de Overfitting\")\n",
    "plt.xlabel(\"Tamaño del Conjunto de Entrenamiento\")\n",
    "plt.ylabel(\"Diferencia de Puntuaciones (Entrenamiento - Prueba)\")\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(train_sizes, overfitting_measure, 'o-', color=\"b\", label=\"Overfitting Measure\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.axhline(y=0, color='r', linestyle='--')  # Línea de referencia en 0\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Imprime la medida de overfitting para el último tamaño de conjunto de entrenamiento\n",
    "print(\"Medida de Overfitting para el último tamaño de conjunto de entrenamiento:\", overfitting_measure[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Entrena tu modelo en el conjunto de entrenamiento\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Realiza predicciones en el conjunto de entrenamiento y prueba\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "\n",
    "# Calcula la precisión en el conjunto de entrenamiento y prueba\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Calcula el overfitting como la diferencia porcentual entre la precisión en entrenamiento y prueba\n",
    "overfitting_percentage = (train_accuracy - test_accuracy) * 100\n",
    "\n",
    "print(f\"Overfitting Percentage: {overfitting_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F5-Airlines-CuSJDrj7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
